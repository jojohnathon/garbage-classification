{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10182596,"sourceType":"datasetVersion","datasetId":2814684}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaydendrino/garbage-classification-transfer-learning?scriptVersionId=221415456\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Settings","metadata":{"id":"vsUbKR0lQ3vd"}},{"cell_type":"markdown","source":"In this notebook we work with the problem of multi-class classification, that is, the correct assignment of an object to one of the classes. Our goal is to achieve high accuracy of assignment to the correct category. We will use the widely used transfer learning method.\n","metadata":{"id":"9u-iEr9kQ7FU"}},{"cell_type":"markdown","source":"Transfer learning is a machine learning technique that uses a pre-trained model. In this approach, a model trained on one data is used or retrained for use with another set of data. By using transfer learning, you can achieve significantly higher classification accuracy, especially when the available data set is limited.","metadata":{}},{"cell_type":"markdown","source":"Before running the code, we need to make sure that we are using the GPU. We plan to work with a convolutional neural network, and using a GPU allows us to reduce training time by several times.","metadata":{}},{"cell_type":"code","source":"# display information about available GPU devices\n!nvidia-smi","metadata":{"id":"oVSPUzXQ2acG","outputId":"b34dfb32-bc8e-4c11-8364-5c3841493e34","execution":{"iopub.status.busy":"2025-02-08T04:38:42.477473Z","iopub.execute_input":"2025-02-08T04:38:42.477824Z","iopub.status.idle":"2025-02-08T04:38:43.8141Z","shell.execute_reply.started":"2025-02-08T04:38:42.477789Z","shell.execute_reply":"2025-02-08T04:38:43.813235Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sat Feb  8 04:38:43 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Import libraries","metadata":{"id":"IbIAPBJODDAz"}},{"cell_type":"code","source":"# Visualization purpose\n!pip install tensorboardX\n!pip install tensorboard\n!pip install tensorboard-plugin-customizable-plots","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:03:06.644795Z","iopub.execute_input":"2025-02-07T22:03:06.645151Z","iopub.status.idle":"2025-02-07T22:03:32.215942Z","shell.execute_reply.started":"2025-02-07T22:03:06.645118Z","shell.execute_reply":"2025-02-07T22:03:32.215084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install split-folders\n!pip install optuna\n!pip install --upgrade nvidia-ml-py3\n","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:03:40.487976Z","iopub.execute_input":"2025-02-07T22:03:40.488334Z","iopub.status.idle":"2025-02-07T22:04:06.1514Z","shell.execute_reply.started":"2025-02-07T22:03:40.488303Z","shell.execute_reply":"2025-02-07T22:04:06.15023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nimport shutil\nimport splitfolders\nfrom pathlib import Path\nimport imghdr\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport matplotlib.pyplot as plt \nimport random\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:04:56.450782Z","iopub.execute_input":"2025-02-07T22:04:56.451255Z","iopub.status.idle":"2025-02-07T22:04:56.456472Z","shell.execute_reply.started":"2025-02-07T22:04:56.451223Z","shell.execute_reply":"2025-02-07T22:04:56.455492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Description of the data set","metadata":{}},{"cell_type":"markdown","source":"The set used has about 22,000 images of various types of waste, divided into 10 different categories - metal, glass, biological waste, etc.","metadata":{}},{"cell_type":"markdown","source":"\n# Data Preprcessing","metadata":{"id":"eEHoivmkDuxP"}},{"cell_type":"code","source":"data_dir = '/kaggle/input/garbage-classification-v2/'\ntrash_dir = '/kaggle/input/garbage-classification-v2/garbage-dataset/trash/'\npaper_dir = '/kaggle/input/garbage-classification-v2/garbage-dataset/paper/'\nmetal_dir = '/kaggle/input/garbage-classification-v2/garbage-dataset/metal/'","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:05:05.127978Z","iopub.execute_input":"2025-02-07T22:05:05.128635Z","iopub.status.idle":"2025-02-07T22:05:05.132548Z","shell.execute_reply.started":"2025-02-07T22:05:05.128606Z","shell.execute_reply":"2025-02-07T22:05:05.131702Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cleaning the working directory if there are any files in it","metadata":{}},{"cell_type":"code","source":"def remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\n#os.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:10:56.8273Z","iopub.execute_input":"2025-02-07T22:10:56.827944Z","iopub.status.idle":"2025-02-07T22:10:56.833453Z","shell.execute_reply.started":"2025-02-07T22:10:56.827911Z","shell.execute_reply":"2025-02-07T22:10:56.83256Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Copying the dataset to the working directory to allow data cleaning","metadata":{}},{"cell_type":"code","source":"#shutil.copytree(data_dir, '/kaggle/working/dataset')\nshutil.copytree(trash_dir, '/kaggle/working/dataset')\nshutil.copytree(paper_dir, '/kaggle/working/dataset')\nshutil.copytree(metal_dir, '/kaggle/working/dataset')","metadata":{"execution":{"iopub.status.busy":"2025-02-07T22:19:35.082346Z","iopub.execute_input":"2025-02-07T22:19:35.082708Z","iopub.status.idle":"2025-02-07T22:19:35.151727Z","shell.execute_reply.started":"2025-02-07T22:19:35.082681Z","shell.execute_reply":"2025-02-07T22:19:35.150527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir  = '/kaggle/working/dataset'","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment_name = \"Transfer Learning\"","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Tensorflow library that we are going to use does not support some image formats, in addition, the dataset may have corrupted or non-image files. Let's pre-clean the dataset.","metadata":{}},{"cell_type":"code","source":"count=0\nimage_extensions = [\".png\", \".jpg\"] \nimg_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n\nfor filepath in Path(data_dir).rglob(\"*\"):\n    if filepath.suffix.lower() in image_extensions:\n        img_type = imghdr.what(filepath)\n        if img_type is None:\n            print(f\"{filepath} is not an image\")\n        if img_type not in img_type_accepted_by_tf:\n            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n            os.remove(filepath)\n            count+=1\nprint(f\"Removed {count} images\")","metadata":{"id":"-WoC2_oyDvZ1","outputId":"475c1552-b876-47ff-9d57-79d81d976810","execution":{"execution_failed":"2025-02-07T21:39:21.064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To train the model, you will need to divide the data into several samples, so we will create folders for them","metadata":{}},{"cell_type":"code","source":"os.mkdir('data')","metadata":{"id":"ueB6tde7SBf9","execution":{"execution_failed":"2025-02-07T21:39:21.065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir('data')","metadata":{"id":"Faefqv6DSx5X","execution":{"execution_failed":"2025-02-07T21:39:21.065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.getcwd()","metadata":{"id":"WdIwUFStS1z7","outputId":"2ebae34c-f70c-446a-d503-fc73df003862","execution":{"execution_failed":"2025-02-07T21:39:21.066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.mkdir('train')\nos.mkdir('test')\nos.mkdir('val')","metadata":{"id":"q95icRCQS4Jt","execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir('/kaggle/working/')","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's set a random value for reproducibility of the result. In particular, when splitting the dataset randomly (as in the following block of code), a fixed SEED value ensures that we get exactly the same split of data next time.","metadata":{}},{"cell_type":"code","source":"SEED = 42","metadata":{"id":"OnPP3rwT2Ved","execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's divide the data from the dataset into three folders - for training and test samples.","metadata":{}},{"cell_type":"code","source":"splitfolders.ratio(\"/kaggle/working/dataset\", output = 'data', seed = SEED, ratio = (.8,.1,.1), group_prefix = None)","metadata":{"id":"jCNLOnYASAdN","outputId":"eac4a526-0bc2-45c5-a1b9-cb36f145bf33","execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can now delete the unused dataset folder to save space.","metadata":{}},{"cell_type":"code","source":"try:\n    shutil.rmtree(data_dir)\nexcept OSError as e:\n    print(\"Error: %s : %s\" % (data_dir, e.strerror))","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Setting paths to training and test data","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/working/data/train'\nval_path = '/kaggle/working/data/val'\ntest_path = '/kaggle/working/data/test'","metadata":{"id":"us5IR8qATvos","execution":{"execution_failed":"2025-02-07T21:39:21.068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualization of data from a dataset","metadata":{"id":"O38W9BwME3Cs"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (5,5)) #выведем 4 изображения\naxes = axes.ravel() \n\nfor i in np.arange(0, 4): \n\n    category = random.choice(os.listdir(train_path)) #случайный выбор класса\n    class_dir = os.path.join(train_path, category)\n\n    image = random.choice(os.listdir(class_dir)) #cлучайный выбор изображения из класса\n \n    img = plt.imread(os.path.join(class_dir,image))\n    axes[i].imshow( img )\n    axes[i].set_title(category) \n    axes[i].axis('off')","metadata":{"id":"u6Jj3OTKU0qS","outputId":"45f10593-76da-4aed-8f01-b044eb6c02a2","execution":{"execution_failed":"2025-02-07T21:39:21.068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at the distribution of data between classes.","metadata":{}},{"cell_type":"code","source":"total = 0\nfor category in os.listdir(train_path):\n    count= 0\n    for image in os.listdir(train_path + \"/\" + category):\n        count += 1\n        total +=1\n    print(str(category).title() + \": \" + str(count))  \nprint(f\"\\nTotal number of train images: {total}\")","metadata":{"id":"ULC2mFLyT7bh","outputId":"3b4cb6ef-9241-4bce-dc6a-cd5ef3a8619d","execution":{"execution_failed":"2025-02-07T21:39:21.068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is noticeable that in some classes the number of images differs several times. Let's visualize the distribution of classes for clarity.","metadata":{}},{"cell_type":"code","source":"# class names\nclass_names = sorted(os.listdir(train_path))\nclass_names","metadata":{"id":"D_QCfavht2A7","outputId":"e198fe3b-3f32-4700-ef5d-80b94abf98f6","execution":{"execution_failed":"2025-02-07T21:39:21.068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_dis = [len(os.listdir(train_path + f\"/{name}\")) for name in class_names]\nclass_dis","metadata":{"id":"OZEESOk2tpJT","outputId":"e959ebe6-e840-4092-e0bb-87de21a813f2","execution":{"execution_failed":"2025-02-07T21:39:21.068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DF = pd.DataFrame(columns=['Class names','Count'])\nDF['Class names']=class_names\nDF['Count']=class_dis\nplt.figure(figsize=(7,4))\nax=sns.barplot(x='Class names', y='Count', data=DF)\nax.bar_label(ax.containers[0])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.tight_layout()","metadata":{"id":"bA1spKdkukCw","outputId":"eee70baa-7b17-4b63-cc4a-f5a0ad5b3b5f","execution":{"execution_failed":"2025-02-07T21:39:21.069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A situation where one or more classes contain a significantly larger number of images is called class imbalance. In our situation, it is natural and determined by the specifics of the data. Thus, there are many more varieties of clothing or glass objects than batteries. However, this may negatively affect the training results of the model and cause it to be biased towards the largest class (that is, it will be much worse at classifying smaller classes). There are different approaches to solving this problem, the applicability of which depends on the problem being solved. We will use the method of insufficient sampling (*random undersampling*), which consists of randomly excluding some examples from large classes.","metadata":{"id":"6uFxM-62UC0-"}},{"cell_type":"markdown","source":"Excluding some random images from the training data","metadata":{}},{"cell_type":"code","source":"for category in os.listdir(train_path):\n    count = 0\n    delete = 0\n    for image in os.listdir(train_path + '/'  + category):\n        count += 1\n        while count > 1000:\n            random_image = random.choice(os.listdir(train_path + '/' + category)) \n            delete_image = train_path + '/'  + category +  '/' + random_image\n            os.remove(delete_image)\n            delete+=1\n            count-=1\n           \n    print(f'Deleted {delete} in {category}')\n","metadata":{"id":"UzFJxgI4gnIB","outputId":"5c3293e5-3140-40da-dab6-67539fb34fa4","execution":{"execution_failed":"2025-02-07T21:39:21.069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We limited the number of images in large classes to 1000. Let's see what the class distribution looks like now","metadata":{}},{"cell_type":"code","source":"class_dis = [len(os.listdir(train_path + f\"/{name}\")) for name in class_names]","metadata":{"id":"Gj9TJVYbv3_H","execution":{"execution_failed":"2025-02-07T21:39:21.069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DF = pd.DataFrame(columns=['class','count'])\nDF['class']=class_names\nDF['count']=class_dis\nplt.figure(figsize=(7,4))\nax=sns.barplot(x='class', y='count', data=DF)\nax.bar_label(ax.containers[0])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.tight_layout()","metadata":{"id":"ET8w6OsjT_HE","outputId":"4ccc4b4e-c0c4-4858-d9d8-3800ac709269","execution":{"execution_failed":"2025-02-07T21:39:21.069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data distribution now looks much better, although some image classes are still quite sparse. We will try to solve this problem using augmentation methods that we integrate into the model.","metadata":{}},{"cell_type":"code","source":"data_path = '/kaggle/working/data'","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To train a neural network, it is necessary to ensure unification of image sizes. To do this, let’s find the average width and height of images in the entire dataset.","metadata":{}},{"cell_type":"code","source":"width = []\nheight = []\nfrom PIL import Image\nimport numpy as np\n\nfor dirname in os.listdir(data_path):\n    for category in os.listdir(data_path + '/' + dirname):\n        for image in os.listdir(data_path + '/' + dirname + '/' + category):\n            img = Image.open(data_path + '/' + dirname + '/' + category + '/' + image)\n            width.append(img.width)\n            height.append(img.height)\n\nprint('Mean width: {:.4f}'.format(np.mean(width)))\nprint('Mean Height: {:.4f}'.format(np.mean(height)))","metadata":{"id":"yL3at6Tobf4t","outputId":"ec307c58-1fc4-44ba-c657-67033607b2dd","execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's set the size of the input data (image size) taking into account the found average values","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (400, 400)","metadata":{"id":"YETLHhbPRPoL","execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's set the size of the data package for loading images into the model in parts","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n","metadata":{"id":"viL4vjPx3S1z","execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a SummaryWriter instance to write logs","metadata":{}},{"cell_type":"code","source":"writer = SummaryWriter()","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Previously, we divided the entire amount of data into three sets. Now you need to prepare them for working with the model - distribute them into packages and add labels to them. Data packets generated from the train folder, constituting 80% of the total number of images, will be used to train the model, from the val(10%) folder - for checking during training and adjusting parameters, and from the test folder - to evaluate the accuracy of the model on new data not used during training.\n","metadata":{"id":"ILLDQ8JGRw73"}},{"cell_type":"code","source":"train_data = tf.keras.utils.image_dataset_from_directory(train_path,\n                                                         image_size=IMG_SIZE,\n                                                         label_mode='categorical',\n                                                         batch_size=BATCH_SIZE,\n                                                         shuffle=True,\n                                                         seed=SEED)","metadata":{"id":"q9FfKNZt2IyR","outputId":"b62d7602-ea0c-480c-9333-022d75b9b3ff","execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = tf.keras.utils.image_dataset_from_directory(test_path,\n                                                        image_size=IMG_SIZE,\n                                                        label_mode='categorical',\n                                                        batch_size=BATCH_SIZE,\n                                                        shuffle=False)","metadata":{"id":"x7j2GWtsnQVc","outputId":"aac04b41-a5a6-4ec4-a205-25dceab66889","execution":{"execution_failed":"2025-02-07T21:39:21.07Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_data = tf.keras.utils.image_dataset_from_directory(val_path,\n                                                              image_size=IMG_SIZE,\n                                                              label_mode='categorical',\n                                                              batch_size=BATCH_SIZE,\n                                                              shuffle=True,\n                                                              seed=SEED)","metadata":{"id":"8pOrlURenWTU","outputId":"c413bc4d-a62c-496d-dac0-a74851d5c9e3","execution":{"execution_failed":"2025-02-07T21:39:21.071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create directories for train and test logs and write logs to it ","metadata":{}},{"cell_type":"code","source":"train_log_dir = os.path.join(\"logs\", experiment_name, \"train\")\ntest_log_dir = os.path.join(\"logs\", experiment_name, \"test\")\n\nos.makedirs(train_log_dir, exist_ok=True)\nos.makedirs(test_log_dir, exist_ok=True)\n\ntrain_writer = SummaryWriter(train_log_dir)\ntest_writer = SummaryWriter(test_log_dir)\n","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building the model","metadata":{"id":"I5aE_6F2GzbB"}},{"cell_type":"markdown","source":"When selecting a model, we examined the classification models available in Keras with pre-trained weights on the public ImageNet dataset, which includes about 1000 categories of various images (https://keras.io/api/applications/).\nFor our task, the most important parameter is the classification accuracy. Based on the accuracy values presented in the table (acc1, percentage of correct answers), the EfficientNetV2S model, which has 88 layers, was selected. Although the EfficientNetV2M and EfficientNetV2L models have higher accuracy, they have significantly higher weights.\n","metadata":{"id":"tCo5FeboGGYp"}},{"cell_type":"markdown","source":"Loading the model","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.EfficientNetV2S(include_top=False,\n                                                   weights='imagenet', \n                                                   input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))","metadata":{"id":"yu4rU-XK3_GO","outputId":"9154f97a-04a4-41c3-a64d-f2f215c1fa4a","execution":{"execution_failed":"2025-02-07T21:39:21.071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see what the architecture of the pretrained model looks like","metadata":{}},{"cell_type":"code","source":"base_model.summary()","metadata":{"id":"dZwuSdeL3_va","outputId":"aeebf4f6-1ba8-42f6-f34b-eb2e0fd73fa4","execution":{"execution_failed":"2025-02-07T21:39:21.071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The architecture of the model we are using already has a Rescale layer, so we may not include an additional image normalization layer in our model.ь.","metadata":{}},{"cell_type":"markdown","source":"Let's freeze all layers of the pretrained model so that its parameters do not change during training.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = False","metadata":{"id":"dp1SuwIE4MST","execution":{"execution_failed":"2025-02-07T21:39:21.071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's create a data augmentation layer to improve classification accuracy. This layer is active only during model training.","metadata":{"id":"IGC7aXMfS1AO"}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal\"),\n                                         tf.keras.layers.RandomRotation(0.2), \n                                         tf.keras.layers.RandomZoom(0.2),\n                                         tf.keras.layers.RandomHeight(0.2),\n                                         tf.keras.layers.RandomWidth(0.2),],\n                                         name =\"data_augmentation\")","metadata":{"id":"5szOnYzudMLZ","execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see how this layer works with images from the training set.","metadata":{}},{"cell_type":"code","source":"for image, _ in train_data.take(1):  \n    plt.figure(figsize=(5, 5))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] / 255)\n        plt.axis('off')","metadata":{"id":"bCeyDOQ-etzq","outputId":"2f74cbc2-b2fa-47e1-cf1b-20914430a91a","execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's combine the model with the augmentation layer, and also add several layers:\n\n* GlobalAveragePooling2D layer, which calculates the arithmetic average over all channels to combine feature maps\n* A fully connected layer with 128 neurons and a Dropout layer, which excludes some neurons from the previous fully connected layer with a given probability, thereby reducing the possible effect of overfitting.\n* Output fully connected layer with the number of outputs corresponding to the number of classes of our data.","metadata":{}},{"cell_type":"code","source":"name=\"EfficientNetV2S\"\n\nEfficientNetV2S_model=tf.keras.Sequential([tf.keras.Input(shape=(None, None, 3), name=\"input_layer\"),\n                    data_augmentation,\n                    base_model,\n                    tf.keras.layers.GlobalAveragePooling2D(),\n                    tf.keras.layers.Dense(128, activation='relu'),\n                    tf.keras.layers.Dropout(0.2),\n                    tf.keras.layers.Dense(len(class_names), activation='softmax')\n                    ], name=name)","metadata":{"id":"J6Kqa_yWA3BC","execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For training, we will define a loss function, an optimizer, and a tracked accuracy metric","metadata":{}},{"cell_type":"code","source":"EfficientNetV2S_model.compile(loss='categorical_crossentropy',\n                              optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n                              metrics=['accuracy'])","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at the structure of our model","metadata":{}},{"cell_type":"code","source":"EfficientNetV2S_model.summary()","metadata":{"id":"eK8W5Phq2fR6","outputId":"0cb37142-ca26-45b3-adfd-0a4359107625","execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The summary already shows the advantage of using a pre-trained model - out of more than 20 million parameters, only 165 thousand are trainable, which will significantly reduce training time.","metadata":{}},{"cell_type":"markdown","source":"While training the models, we will use a callback function to stop training when the metrics being tracked stop improving. After this, we will save the trained model and weights for further use.","metadata":{}},{"cell_type":"code","source":"# def log_to_tensorboard(epoch, logs):\n#         train_writer.add_scalar('loss', logs['loss'], epoch)\n#         train_writer.add_scalar('accuracy', logs['accuracy'], epoch)\n#         test_writer.add_scalar('val_loss', logs['val_loss'], epoch)\n#         test_writer.add_scalar('val_accuracy', logs['val_accuracy'], epoch)\ndef log_to_tensorboard(epoch, logs):\n    train_writer.add_scalar('loss', logs['loss'], epoch)\n    train_writer.add_scalar('accuracy', logs['accuracy'], epoch)\n    \n    # Use get method with a default value of None to avoid KeyError\n    val_loss = logs.get('val_loss', None)\n    val_accuracy = logs.get('val_accuracy', None)\n    \n    if val_loss is not None:\n        test_writer.add_scalar('val_loss', val_loss, epoch)\n    if val_accuracy is not None:\n        test_writer.add_scalar('val_accuracy', val_accuracy, epoch)\n\n\n# Creating a Callback Function\ndef create_callback(model_name):\n    \n    # termination of training when accuracy does not improve within 3 epochs\n    early_stop = tf.keras.callbacks.EarlyStopping(patience=3,\n                                                monitor=\"val_loss\",\n                                                mode=\"min\",\n                                                verbose=1)\n\n    # decrease in learning rate when the error rate does not decrease over 2 epochs\n    reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                              factor=0.2,\n                                                              patience=2,\n                                                              verbose=1,\n                                                              min_lr=0.00001)\n\n    log_to_tensorboard_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_to_tensorboard(epoch, logs))\n    # saving model\n    check_model = tf.keras.callbacks.ModelCheckpoint(model_name + \".h5\",\n                                                   monitor=\"val_accuracy\",\n                                                   mode=\"max\",\n                                                   save_best_only=True)\n    callback = [early_stop, reduce_learning_rate, check_model, log_to_tensorboard_callback]\n    return callback","metadata":{"id":"ERnRJA_87r2t","execution":{"execution_failed":"2025-02-07T21:39:21.072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EfficientNetV2S_callback = create_callback(name)","metadata":{"id":"YqRNxjQh5EUx","execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's set the number of training epochs","metadata":{}},{"cell_type":"code","source":"EPOCH=20","metadata":{"id":"JZgRKxTchBdT","execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's train the model","metadata":{}},{"cell_type":"code","source":"import time\nstart_time = time.time()\nEfficientNetV2S_history = EfficientNetV2S_model.fit(train_data, \n                                                    epochs=EPOCH, \n                                                    steps_per_epoch=len(train_data),\n                                                    validation_data=validation_data, \n                                                    validation_steps=len(validation_data),\n                                                    callbacks=EfficientNetV2S_callback)","metadata":{"id":"5qIdQpt74x0d","outputId":"75f306a2-5fb9-49e0-8db9-091a2fba9de9","execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the training history, you can evaluate the effectiveness of using our callback function - after reducing the learning speed, the errors also decreased.","metadata":{}},{"cell_type":"markdown","source":"Out the logs folder for further analysis","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Create a zip archive of the output folder\nshutil.make_archive(\"/kaggle/working/logs\", 'zip', \"/kaggle/working/logs\")","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at the model training schedules.","metadata":{}},{"cell_type":"code","source":"end_time = time.time()\ntraining_time = end_time - start_time\nprint(\"Total training time: {:.2f} seconds\".format(training_time))\nEfficientNetV2S_model.save(\"/kaggle/working/garbage-classification.h5\")","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Close the SummaryWriter","metadata":{}},{"cell_type":"code","source":"writer.close()\ntrain_writer.close()\ntest_writer.close()","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EfficientNetV2S_history.history.keys()","metadata":{"id":"ltAkXMJl_v0W","outputId":"83284b23-6363-49d5-9972-f5a9fcefbd47","execution":{"execution_failed":"2025-02-07T21:39:21.074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for displaying training graphs\ndef plot_loss_curves(history):\n\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    \n    accuracy = history.history[\"accuracy\"]\n    val_accuracy = history.history[\"val_accuracy\"]\n    \n    epochs = range(len(history.history[\"loss\"]))\n    \n    plt.plot(epochs, loss, label=\"training_loss\")\n    plt.plot(epochs, val_loss, label=\"val_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    \n    plt.figure()\n    plt.plot(epochs, accuracy, label=\"training_accuracy\")\n    plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()","metadata":{"id":"hEigRh5ZCBaQ","execution":{"execution_failed":"2025-02-07T21:39:21.074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss_curves(EfficientNetV2S_history)","metadata":{"id":"McOrDCmXARjd","execution":{"execution_failed":"2025-02-07T21:39:21.074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The learning curves look a little jumpy, but the dynamics look pretty good, there is a decrease in learning losses and an increase in testing accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"We use a set of test data to evaluate the accuracy of the model on new data","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = EfficientNetV2S_model.evaluate(test_data, verbose=0)","metadata":{"id":"ysyA_N5-HB1l","execution":{"execution_failed":"2025-02-07T21:39:21.076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Test Loss: {:.5f}\".format(test_loss))\nprint(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))","metadata":{"id":"h9LbofxnHNlA","execution":{"execution_failed":"2025-02-07T21:39:21.076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Given the limited data set, we obtained a good accuracy of 97%.","metadata":{}},{"cell_type":"markdown","source":"Let's look at the main classification metrics","metadata":{}},{"cell_type":"code","source":"pred_probs = EfficientNetV2S_model.predict(test_data, verbose=1)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_classes = pred_probs.argmax(axis=1)\npred_classes[:10]","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_labels = []\nfor images, labels in test_data.unbatch(): \n    y_labels.append(labels.numpy().argmax()) \ny_labels[:10] ","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the images in the test dataset were not shuffled, all the output labels correspond to the same class.","metadata":{}},{"cell_type":"code","source":"print('Classification Report \\n')\ntarget_names = class_names\nprint(classification_report(y_labels, pred_classes, target_names=target_names))","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* The “Precision” column shows the percentage of correct predictions among all predictions for a particular class.\n* The \"Recall\" column shows the percentage of images of a particular class that were predicted by the model to belong to that class.\n* Column “F1” shows what percentage of model predictions were correct.\n* The \"Support\" column shows how many images of each class were in the test dataset.","metadata":{}},{"cell_type":"markdown","source":"From the summary of metrics it is clear that the model classifies plastic worst of all, and best of all - things and various biological waste. However, despite the small data set, the model shows excellent performance in terms of Precision, Recall and F1 scores.","metadata":{}},{"cell_type":"markdown","source":"We visualize the error matrix to clearly see the correctness of class definitions","metadata":{}},{"cell_type":"code","source":"import itertools","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes):\n    plt.figure(figsize=(7,7))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    cm = np.around(cm, decimals=2)\n    cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_labels, pred_classes)\nplot_confusion_matrix(cm, class_names)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The rows of the matrix are responsible for the class predicted by the model, and the columns are for the actual class. From the error matrix, it is noticeable that in general the model’s errors are very small, although it sometimes incorrectly predicts categories, for example, it confuses plastic and glass.","metadata":{}},{"cell_type":"markdown","source":"Visualizing several model predictions","metadata":{}},{"cell_type":"code","source":"def pred_random_images(model_name, folder_path, class_names):\n    plt.figure(figsize=(17, 10))\n    for i in range(3):\n        class_name = random.choice(class_names)\n        filename = random.choice(os.listdir(folder_path + \"/\" + class_name))\n        filepath = folder_path + \"/\" + class_name + \"/\" + filename\n    \n        img = tf.io.read_file(filepath)\n        img = tf.image.decode_jpeg(img)\n        img = tf.image.resize(img, [IMG_SIZE[0], IMG_SIZE[1]])\n     #  img = img/255.\n        pred_prob = model_name.predict(tf.expand_dims(img, axis=0), verbose=0) \n        pred_class = class_names[pred_prob.argmax()] \n\n  # Plot the image(s)\n        plt.subplot(1, 3, i+1)\n        plt.imshow(img/255.)\n        if class_name == pred_class: \n            title_color = \"g\"\n        else:\n            title_color = \"r\"\n        plt.title(f\"Class: {class_name},\\n Pred: {pred_class},\\n Prob: {pred_prob.max():.2f}\", c=title_color)\n        plt.axis(False);","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_random_images(EfficientNetV2S_model, test_path, class_names)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\npickle.dump(EfficientNetV2S_model,open(\"EfficientNetV2S_model.h5\",\"wb\"))\n# EfficientNetV2S_model.save('EfficientNetV2S_model.h5')","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls ../working","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tunning","metadata":{}},{"cell_type":"markdown","source":"We use Optuna for hyperparameter tunning. Here we are defining objective ","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    # hyperparameters to optimize\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n    \n    # Update the model with the suggested hyperparameters\n    EfficientNetV2S_model = tf.keras.Sequential([\n        tf.keras.Input(shape=(None, None, 3), name=\"input_layer\"),\n        data_augmentation,\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(dropout_rate),  # Use the suggested dropout_rate directly\n        tf.keras.layers.Dense(len(class_names), activation='softmax')\n    ], name=name)\n    \n    # Update the model with the suggested hyperparameters\n    EfficientNetV2S_model.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        metrics=['accuracy']\n    )\n\n    # Train the model with the updated hyperparameters\n    history = EfficientNetV2S_model.fit(\n        train_data,\n        epochs=EPOCH,\n        steps_per_epoch=len(train_data),\n        validation_data=validation_data,\n        validation_steps=len(validation_data),\n        callbacks=EfficientNetV2S_callback\n    )\n\n    # Return the validation accuracy as the objective value to minimize\n    return -history.history['val_accuracy'][-1]","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # You can adjust the number of trials\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from optuna.visualization import plot_optimization_history\n\nplot_optimization_history(study)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the final model with the best hyperparameters\nfinal_model = tf.keras.Sequential([\n    tf.keras.Input(shape=(None, None, 3), name=\"input_layer\"),\n    data_augmentation,\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(best_params['dropout_rate']),  # Use the best dropout_rate\n    tf.keras.layers.Dense(len(class_names), activation='softmax')\n], name=name)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.08Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update the model with the best hyperparameters\nfinal_model.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n    metrics=['accuracy']\n)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.08Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model with the entire dataset or a larger portion if needed\nfinal_epochs = 20  # same epochs for consistency\nfinal_history = final_model.fit(\n    train_data,\n    epochs=final_epochs,\n    steps_per_epoch=len(train_data),\n    callbacks=EfficientNetV2S_callback\n)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.08Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_accuracy = final_model.evaluate(test_data, verbose=0)","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.08Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Test Loss: {:.5f}\".format(test_loss))\nprint(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.08Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save the result\npickle.dump(final_model,open(\"EfficientNetV2S_model_optimized.h5\",\"wb\"))","metadata":{"execution":{"execution_failed":"2025-02-07T21:39:21.081Z"},"trusted":true},"outputs":[],"execution_count":null}]}